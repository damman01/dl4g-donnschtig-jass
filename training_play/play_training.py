import os

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

import logging

from pathlib import Path

import tensorflow as tf
import tensorflow.keras.mixed_precision as mixed_precision
from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping
from tensorflow import keras

from play_training_data_prep_json import get_train_data

# add a constant value for the shape of the input data
INPUT_FEATURES = 46


# This function generates training data from a list of files
def data_generator(list_files: list, batch_size: int = 36):
    # Convert PosixPath objects to strings
    list_files = [str(path) for path in list_files]

    files_dataset = tf.data.Dataset.from_tensor_slices(list_files)
    dataset = files_dataset.interleave(
        lambda file_: tf.data.TextLineDataset(file_).map(
            lambda x: tf.py_function(func=lambda x: get_train_data(x.numpy().decode('utf-8')), inp=[x],
                                     Tout=(tf.float32, tf.float32), name=None),
            num_parallel_calls=tf.data.experimental.AUTOTUNE),
        cycle_length=tf.data.experimental.AUTOTUNE,
        num_parallel_calls=tf.data.experimental.AUTOTUNE
    )

    # Define the output shapes
    dataset = dataset.map(lambda x, y: (tf.reshape(x, (INPUT_FEATURES,)), tf.reshape(y, (36,))))

    # Shuffle and batch the dataset
    dataset = dataset.batch(batch_size)

    return dataset.cache().prefetch(tf.data.experimental.AUTOTUNE)

    # while True:
    #     for file_name in list_files:
    #         with open(file_name, 'r') as file:
    #             print("\nReading file:", file_name)
    #             lines = file.readlines()
    #             random.shuffle(lines)
    #             for i in range(0, len(lines), batch_size):
    #                 batch_lines = lines[i:i+batch_size]
    #                 data = get_train_data(batch_lines)
    #                 x_train = pd.concat(data[0], ignore_index=True).values
    #                 y_categorical_data_train = keras.utils.to_categorical(data[1])
    #                 yield x_train, y_categorical_data_train


# This function trains the model using the data generated by the data_generator function
def train_model(learning_model, train_dataset, val_dataset):
    if tf.test.gpu_device_name():
        print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))
    else:
        print("Please install GPU version of TF by running  'python3 -m pip install tensorflow[and-cuda]' in your WSL "
              "Environment!")
        raise Exception("Please install GPU version of TF by running  'python3 -m pip install tensorflow[and-cuda]'")

    # open TensorBoard with: 'tensorboard --logdir=logs' in separate terminal
    tb_callback = TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True)

    checkpoint_path = "../models/checkpoints/cp-{epoch:04d}.keras"
    checkpoint_dir = os.path.dirname(checkpoint_path)

    cp_callback = ModelCheckpoint(filepath=checkpoint_path,
                                  verbose=1,
                                  save_weights_only=False,
                                  save_freq='epoch')  # Save weights, every epoch.

    logger.info("Starting training...")  # Log the start of training

    early_stopping = EarlyStopping(
        monitor='val_loss',
        min_delta=0.001,
        patience=5,
        verbose=1,
        restore_best_weights=True
    )

    model_history = learning_model.fit(
        train_dataset,
        epochs=40,
        callbacks=[cp_callback, tb_callback, early_stopping],
        validation_data=val_dataset
    )

    logger.info("Training completed.")  # Log the end of training

    return model_history


def create_model():
    training_model = keras.Sequential([
        keras.layers.Input(shape=(INPUT_FEATURES,)),
        keras.layers.Dense(32, activation='relu'),
        keras.layers.Dense(64, activation='relu'),
        # keras.layers.BatchNormalization(),  # BatchNormalization: These layers can help accelerate training.
        # keras.layers.Dropout(rate=0.2),
        # keras.layers.Dense(1600, activation='relu'),
        keras.layers.Dense(36, activation='softmax')
    ])
    if policy.name == 'mixed_float16':
        training_model.add(keras.layers.Dense(36, activation='softmax', dtype='float32'))

    # optimizer = keras.optimizers.RMSprop(learning_rate=0.001)
    optimizer = keras.optimizers.Adam(learning_rate=0.01)
    training_model.compile(loss='categorical_crossentropy',
                           optimizer=optimizer,
                           metrics=['accuracy', 'Precision', 'Recall'])

    return training_model


# Main Method for training the model
# 36901728 lines will be read
if __name__ == "__main__":
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    script_dir = os.path.dirname(os.path.abspath(__file__))
    # paths_to_data = os.path.join(script_dir, "./gamelogs/")
    # paths_to_data = os.path.join(script_dir, "./gamelogs/jass_game_0001/")
    paths_to_data = os.path.join(script_dir, "./test_files/jass_game_0001/")

    # Set up mixed precision
    policy = mixed_precision.Policy('mixed_float16')
    mixed_precision.set_global_policy(policy)

    # Create a MirroredStrategy for distributed training
    if tf.config.list_physical_devices('GPU'):
        strategy = tf.distribute.MirroredStrategy()
    else:
        strategy = None

    # Model creation
    with strategy.scope():
        model = create_model()

    print('Searching directory...:', paths_to_data)
    file_list = []

    for file in Path(paths_to_data).rglob('*.txt'):
        file_list.append(file)
    file_list.sort()
    print('Found {} files.'.format(len(file_list)))

    train_files = file_list[:int(len(file_list) * 0.7)]
    val_files = file_list[int(len(file_list) * 0.8):]
    test_files = file_list[int(len(file_list) * 0.9):]
    print("Train files:", len(train_files))
    print("Val files:", len(val_files))
    print("Test files:", len(test_files))

    # Create the generators
    train_gen = data_generator(train_files)

    val_gen = data_generator(val_files)

    # Train the model
    history = train_model(model, train_gen, val_gen)
    logger.info("Trained model with history:")
    logger.info(history)

    # Save the model
    model.save('../models/playModelTest.keras')

    val_gen = data_generator(val_files)
    # Evaluate the model on the validation set
    score = model.evaluate(val_gen, steps=len(val_files))
    logger.info("Evaluation on validation set:")
    logger.info(score)

    # Test the model on new data
    test_gen = data_generator(test_files)

    test_loss, test_acc, test_precision, test_recall = model.evaluate(test_gen, steps=len(test_files))
    logger.info("Testing on new data:")
    logger.info(f"Test loss: {test_loss}")
    logger.info(f"Test accuracy: {test_acc}")
    logger.info(f"Test precision: {test_precision}")
    logger.info(f"Test recall: {test_recall}")
